"""Dataset utilities for LAMF training.

This module provides PyTorch dataset classes and data loading utilities
for LAMF training. It reuses the data generated by ml_init/dataset.py.
"""
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
from typing import Optional, Union


class LAMFDataset(Dataset):
    """
    PyTorch Dataset for LAMF training.
    
    Loads pre-generated PDF data and converts to probability mass (w).
    """
    
    def __init__(
        self,
        data_path: Union[str, Path],
        normalize_w: bool = True,
    ):
        """
        Initialize LAMF dataset.
        
        Parameters:
        -----------
        data_path : str or Path
            Path to .npz file containing:
            - z: Grid points, shape (N,)
            - f: PDF values, shape (n_samples, N)
            - params: Parameters, shape (n_samples, 5)
        normalize_w : bool
            If True, normalize w to sum to 1
        """
        self.data_path = Path(data_path)
        self.normalize_w = normalize_w
        
        # Load data
        data = np.load(self.data_path)
        self.z = data['z'].astype(np.float32)  # (N,)
        self.f = data['f'].astype(np.float32)  # (n_samples, N)
        self.params = data['params'].astype(np.float32)  # (n_samples, 5)
        
        self.N = len(self.z)
        self.n_samples = len(self.f)
        
        # Pre-compute probability mass w = f * dz
        # Using trapezoidal rule weights
        dz = self.z[1] - self.z[0] if self.N > 1 else 1.0
        weights = np.full(self.N, dz, dtype=np.float32)
        if self.N > 1:
            weights[0] = weights[-1] = dz / 2
        
        self.w = self.f * weights  # (n_samples, N)
        
        if normalize_w:
            # Normalize each sample to sum to 1
            sums = self.w.sum(axis=1, keepdims=True)
            sums = np.maximum(sums, 1e-12)  # Avoid division by zero
            self.w = self.w / sums
        
        # Store grid info
        self.z_min = float(self.z[0])
        self.z_max = float(self.z[-1])
        self.dz = dz
    
    def __len__(self) -> int:
        return self.n_samples
    
    def __getitem__(self, idx: int) -> dict:
        """
        Get a single sample.
        
        Returns:
        --------
        sample : dict
            Dictionary containing:
            - z: Grid points, shape (N,)
            - w: Probability mass, shape (N,)
            - f: PDF values (original), shape (N,)
            - params: Parameters (mu_x, sigma_x, mu_y, sigma_y, rho), shape (5,)
        """
        return {
            'z': torch.from_numpy(self.z),
            'w': torch.from_numpy(self.w[idx]),
            'f': torch.from_numpy(self.f[idx]),
            'params': torch.from_numpy(self.params[idx]),
        }
    
    def get_z_tensor(self) -> torch.Tensor:
        """Get grid points as tensor."""
        return torch.from_numpy(self.z)


def create_dataloaders(
    data_dir: Union[str, Path],
    batch_size: int = 64,
    num_workers: int = 4,
    pin_memory: bool = True,
) -> tuple[DataLoader, DataLoader, DataLoader, dict]:
    """
    Create train/val/test dataloaders.
    
    Parameters:
    -----------
    data_dir : str or Path
        Directory containing train.npz, val.npz, test.npz
    batch_size : int
        Batch size for training
    num_workers : int
        Number of data loading workers
    pin_memory : bool
        Pin memory for faster GPU transfer
    
    Returns:
    --------
    train_loader : DataLoader
        Training dataloader
    val_loader : DataLoader
        Validation dataloader
    test_loader : DataLoader
        Test dataloader
    metadata : dict
        Dictionary containing N, z_min, z_max, z (grid)
    """
    data_dir = Path(data_dir)
    
    # Load datasets
    train_dataset = LAMFDataset(data_dir / "train.npz")
    val_dataset = LAMFDataset(data_dir / "val.npz")
    test_dataset = LAMFDataset(data_dir / "test.npz")
    
    # Create dataloaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=pin_memory,
        drop_last=True,
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=pin_memory,
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=pin_memory,
    )
    
    # Metadata
    metadata = {
        'N': train_dataset.N,
        'z_min': train_dataset.z_min,
        'z_max': train_dataset.z_max,
        'z': train_dataset.z,
        'n_train': len(train_dataset),
        'n_val': len(val_dataset),
        'n_test': len(test_dataset),
    }
    
    return train_loader, val_loader, test_loader, metadata


def collate_fn(batch: list[dict]) -> dict:
    """
    Custom collate function for LAMF dataset.
    
    Parameters:
    -----------
    batch : list of dict
        List of samples from __getitem__
    
    Returns:
    --------
    collated : dict
        Batched tensors:
        - z: Grid points, shape (N,) - same for all samples
        - w: Probability mass, shape (batch_size, N)
        - f: PDF values, shape (batch_size, N)
        - params: Parameters, shape (batch_size, 5)
    """
    return {
        'z': batch[0]['z'],  # Same for all samples
        'w': torch.stack([b['w'] for b in batch]),
        'f': torch.stack([b['f'] for b in batch]),
        'params': torch.stack([b['params'] for b in batch]),
    }

