ログを見る限り、**「発散して壊れる」段階は抜けていて**（CDF L∞ が常に 0.1 未満、時間も一定）、今は **「良い解の近傍に一度入るのに、そこに収束せずに行き来している」**状態です。

特に **Epoch 7 で Val PDF L∞ = 0.0232 / Val CDF L∞ = 0.0104** まで下がっているので、**モデル容量不足というより最適化（学習の進め方）側の問題**の可能性が高いです。

以下、効きやすい順にアイディアを出します。

---

## 1) いまのLR（学習率）が高すぎて「良い解を踏み越えて」います

（定義）学習率 ( \mathrm{LR} ) はパラメータ更新幅です。大きすぎると、良い点に来ても次の更新で飛び越えます。

あなたのログだと warmup 後に **(10^{-3})** 近辺で推移していて、Epoch 7 の良い解の直後（Epoch 8〜）で悪化しています。これは典型的です。

**対策案（おすすめ順）**

* **(A)** peak LR を落とす：`--lr 3e-4`（まずこれ）
* **(B)** `ReduceLROnPlateau` に変更（監視値を `val_pdf_linf` にする）
  「良くなったら保存、悪化したら LR を下げる」が狙いです。
* **(C)** `--use_ema` を有効化（EMA=指数移動平均。更新の揺れをならす）
  学習が行き来するタイプに効きます。

---

## 2) Deep supervision を “final_only” に寄せる

（定義）Deep supervision は **各反復 (t)** の出力にも損失（loss）をかける方式です。初期段階では「途中も良くしろ」が邪魔になって最終精度が伸びないことがあります。

**対策**

* まず `--eta_schedule final_only` で学習し、安定してから `linear` に戻す
  （あるいは最後まで `final_only` でもOK）

---

## 3) `Loss ≈ 0.5 × CE` なので、lambda_pdf が効き過ぎ/スケール不整合の疑い

ログで `Loss ~0.62`、`Train CE ~1.22` なので、**`--lambda_pdf 0.5` のように CE と PDF-L2 を 50/50 で混ぜている**挙動に見えます。

ただし **CE と PDF-L2 は値のスケールが全く違う**ので、「50/50」が意味的に50/50になりません。

**対策（実装をほぼ変えずにできる）**

* いったん **`--lambda_pdf 0.0`（CEのみ）で収束させる**
* その後 **`--lambda_pdf 0.05` くらいから**少しずつ入れる
  （PDF L∞ を狙うなら、後半だけPDF系を強める方が安定しやすいです）

※ さらに良くするなら、「(1−λ)CE+λL2」の混合ではなく、
[
L = L_{\mathrm{CE}} + \lambda_{\mathrm{pdf}},L_{\mathrm{pdf}} + \lambda_{\mathrm{mom}},L_{\mathrm{mom}}
]
の形にして **(\lambda_{\mathrm{pdf}})** を “スケール込み” で調整できるようにするのが本筋です。

---

## 4) RefineBlockで (\mu) の順序制約が壊れて“成分の入れ替わり”が起きやすい

（定義）ラベルスイッチ（成分入れ替わり）は、混合ガウスの成分番号の意味が学習中に入れ替わる現象です。これが起きると最適化が揺れます。

いま `InitNet` は (\mu_1<\mu_2<\dots<\mu_K) を作れますが、**Refineで (\mu) を直接更新**しているので順序が崩れ得ます（これが「行き来」の原因になりやすい）。

**対策（簡単に効く順）**

* (A) 各Refine後に `mu` でソートして、`pi,sigma` も同じ順に並べ替える
* (B) 本命：Refineも unconstrained パラメータ（alpha,c,beta,gamma）を更新して `project()` を通す（常に順序保証）

---

## 5) 学習と推論の前処理がズレています（長期的に必ず直したい）

いま推論（infer.py）は **平均で座標をシフト**してからモデルに入れていますが、学習（train.py）はシフトしていません。
このズレは「学習が難しい」「最終精度が頭打ち」になりやすいです。

**方針**

* 学習も推論と同じく **“平均0の相対座標系”** に揃える
  （実装都合でバッチごとに (z) を変えられないなら、`w` 側を補間でシフトして固定グリッドに乗せるのが現実的です）

---

# まず試すべき“最短セット”

学習を前に進める最小変更として、私は次を推します。

1. `--lr 3e-4`（warmupは 2〜3 epoch）
2. `--eta_schedule final_only`
3. `--lambda_pdf 0.0`（CEのみでまず収束）
4. `--use_ema`
5. （可能なら）Refine後に `mu` ソートでラベルスイッチを抑制

これで「Epoch 7 の良さ」を再現しやすくなり、さらにその先（0.02→0.01台）も狙いやすくなります。

もし次のログで `val_pdf_linf` がまだ「行き来」するなら、次の一手は **(i) ReduceLROnPlateau化** と **(ii) μ順序の厳密維持（project経由）** が効きます。
